<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="description"
    content="QuantVLA: The first post-training quantization framework for Vision-Language-Action models.">
  <meta name="keywords" content="QuantVLA, Robotics, VLA, Quantization, PTQ, DiT">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>QuantVLA: Quantizing Vision-Language-Action Models</title>

  <!-- Fonts -->
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800;900&display=swap"
    rel="stylesheet">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.0/css/all.min.css">

  <!-- Styles -->
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./image/icon.png">
</head>

<body>

  <!-- ===== Sticky Navbar ===== -->
  <nav class="site-nav" id="siteNav">
    <div class="nav-inner">
      <a href="#" class="nav-brand">
        <img src="./image/icon.png" alt="QuantVLA">
        <span>QuantVLA</span>
      </a>
      <ul class="nav-links">
        <li><a href="#abstract" data-en="Abstract" data-zh="æ‘˜è¦">Abstract</a></li>
        <li><a href="#method" data-en="Method" data-zh="æ–¹æ³•">Method</a></li>
        <li><a href="#memory" data-en="Memory" data-zh="å†…å­˜">Memory</a></li>
        <li><a href="#leaderboard" data-en="Leaderboard" data-zh="æ’è¡Œæ¦œ">Leaderboard</a></li>
        <li><a href="#comparison" data-en="Comparison" data-zh="å¯¹æ¯”">Comparison</a></li>
        <li><a href="#bibtex">BibTeX</a></li>
        <li>
          <button id="langToggle" class="lang-btn" title="Switch Language">
            <i class="fas fa-globe"></i> EN
          </button>
        </li>
      </ul>
    </div>
  </nav>

  <!-- ===== Hero Section ===== -->
  <section class="hero-main">
    <div class="hero-content">

      <!-- "First" Badge -->
      <div class="first-badge" data-en="ğŸ† First Post-Training Quantization for VLA Systems"
        data-zh="ğŸ† é¦–ä¸ªé¢å‘ VLA ç³»ç»Ÿçš„è®­ç»ƒåé‡åŒ–æ–¹æ³•">ğŸ† First Post-Training Quantization for VLA Systems</div>

      <!-- Large Icon + QuantVLA Title -->
      <div class="hero-logo-block">
        <img src="./image/icon.png" alt="QuantVLA Icon">
        <span class="hero-logo-text">QuantVLA</span>
      </div>

      <p class="hero-subtitle"
        data-en="Scale-Calibrated Post-Training Quantization for<br>Vision-Language-Action Models"
        data-zh="é¢å‘è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹çš„<br>å°ºåº¦æ ¡å‡†è®­ç»ƒåé‡åŒ–">
        Scale-Calibrated Post-Training Quantization for<br>
        Vision-Language-Action Models
      </p>

      <!-- Authors -->
      <div class="hero-authors">
        <a href="#">Jingxuan Zhang</a><sup>2â€ </sup>,
        <a href="#">Yunta Hsieh</a><sup>3â€ </sup>,
        <a href="#">Zhongwei Wan</a><sup>1</sup>,
        <a href="#">Haokun Lin</a><sup>4</sup>,
        <a href="#">Xin Wang</a><sup>1</sup>,
        <a href="#">Ziqi Wang</a><sup>1</sup>,
        <a href="#">Yingtie Lei</a><sup>1</sup>,
        <a href="#">Mi Zhang</a><sup>1</sup>*
      </div>
      <div class="hero-affiliations">
        <sup>1</sup>The Ohio State University,
        <sup>2</sup>Indiana University,
        <sup>3</sup>University of Michigan,
        <sup>4</sup>City University of Hong Kong
      </div>
      <div class="hero-note" data-en="<sup>â€ </sup>Equal Contribution" data-zh="<sup>â€ </sup>åŒç­‰è´¡çŒ®">
        <sup>â€ </sup>Equal Contribution
      </div>
      <div style="color: #ffffff; font-size: 1rem; margin-bottom: 1.5rem;">
        * Corresponding Author: mizhang.1@osu.edu
      </div>

      <!-- Action Buttons -->
      <div class="hero-buttons">
        <a href="https://arxiv.org/pdf/2011.12948" class="hero-btn hero-btn-primary">
          <i class="fas fa-file-pdf"></i> Paper
        </a>
        <a href="https://arxiv.org/abs/2011.12948" class="hero-btn hero-btn-outline">
          <i class="ai ai-arxiv"></i> arXiv
        </a>
        <a href="https://github.com/AIoT-MLSys-Lab/QuantVLA" class="hero-btn hero-btn-outline">
          <i class="fab fa-github"></i> Code
        </a>
      </div>
    </div>
  </section>

  <!-- ===== Highlight Cards ===== -->
  <section class="highlights-section">
    <div class="highlights-grid">
      <div class="highlight-card">
        <div class="highlight-icon">ğŸ†</div>
        <div class="highlight-title" data-en="First PTQ for VLA" data-zh="é¦–ä¸ª VLA é‡åŒ–æ–¹æ¡ˆ">First PTQ for VLA</div>
        <div class="highlight-desc"
          data-en="First post-training quantization framework for VLA systems and first to quantize a DiT action head"
          data-zh="é¦–ä¸ªé¢å‘ VLA ç³»ç»Ÿçš„è®­ç»ƒåé‡åŒ–æ¡†æ¶ï¼Œä¹Ÿæ˜¯é¦–ä¸ªæˆåŠŸé‡åŒ– DiT åŠ¨ä½œå¤´çš„æ–¹æ³•">First post-training quantization framework for VLA systems
          and first to quantize a
          DiT action head</div>
      </div>
      <div class="highlight-card">
        <div class="highlight-icon">ğŸ’¾</div>
        <div class="highlight-title" data-en="~70% Memory Savings" data-zh="~70% å†…å­˜èŠ‚çœ">~70% Memory Savings</div>
        <div class="highlight-desc"
          data-en="Significant memory reduction on quantized components with no architecture changes required"
          data-zh="é‡åŒ–ç»„ä»¶æ˜¾è‘—å‡å°‘å†…å­˜å ç”¨ï¼Œæ— éœ€ä»»ä½•æ¶æ„ä¿®æ”¹">Significant memory reduction on quantized components with no architecture
          changes
          required</div>
      </div>
      <div class="highlight-card">
        <div class="highlight-icon">âš¡</div>
        <div class="highlight-title" data-en="Training-Free" data-zh="æ— éœ€è®­ç»ƒ">Training-Free</div>
        <div class="highlight-desc"
          data-en="Uses only a small unlabeled calibration buffer â€” no additional training or fine-tuning needed"
          data-zh="ä»…éœ€å°‘é‡æ— æ ‡æ³¨æ ¡å‡†æ•°æ®ï¼Œæ— éœ€é¢å¤–è®­ç»ƒæˆ–å¾®è°ƒ">Uses only a small unlabeled calibration buffer â€” no additional training or
          fine-tuning needed</div>
      </div>
    </div>
  </section>

  <!-- ===== Video Demos ===== -->
  <section class="demo-section">
    <div class="demo-inner">
      <div class="demo-videos">
        <video autoplay muted loop playsinline>
          <source src="./video/simulation1.mp4" type="video/mp4">
        </video>
        <video autoplay muted loop playsinline>
          <source src="./video/simluation2.mp4" type="video/mp4">
        </video>
        <video autoplay muted loop playsinline>
          <source src="./video/simulation3.mp4" type="video/mp4">
        </video>
        <video autoplay muted loop playsinline>
          <source src="./video/simulation4.mp4" type="video/mp4">
        </video>
      </div>
      <p class="demo-caption"
        data-en="<span class='dnerf'>QuantVLA</span> enables efficient robot learning through quantization."
        data-zh="<span class='dnerf'>QuantVLA</span> é€šè¿‡é‡åŒ–æŠ€æœ¯å®ç°é«˜æ•ˆçš„æœºå™¨äººå­¦ä¹ ã€‚">
        <span class="dnerf">QuantVLA</span> enables efficient robot learning through quantization.
      </p>
    </div>
  </section>

  <!-- ===== Abstract ===== -->
  <section class="content-section" id="abstract">
    <div class="section-inner">
      <h2 class="section-title" data-en="Abstract" data-zh="æ‘˜è¦">Abstract</h2>
      <div class="abstract-box">
        <p data-en="Vision-language-action (VLA) models unify perception, language, and control for embodied agents but face significant challenges in practical deployment due to rapidly increasing compute and memory demands, especially as models scale to longer horizons and larger backbones. To address these bottlenecks, we introduce <b>QuantVLA</b>, a training-free post-training quantization (PTQ) framework that, to our knowledge, is the first PTQ approach for VLA systems and the first to successfully quantize a diffusion transformer (DiT) action head. <b>QuantVLA</b> incorporates three scale-calibrated components: (1) a selective quantization layout that integerizes all linear layers in both the language backbone and the DiT while keeping attention projections in floating point to preserve the original operator schedule; (2) attention temperature matching, a lightweight per-head scaling mechanism that stabilizes attention logits and is folded into the dequantization scales at inference; and (3) output head balancing, a per-layer residual interface calibration that mitigates post-projection energy drift. The framework requires no additional training, uses only a small unlabeled calibration buffer, and supports integer kernels for low-bit weights and activations while leaving the architecture unchanged. Across representative VLA models on LIBERO, <b>QuantVLA</b> exceeds the task success rates of fully-precision baselines, achieves about <b>70%</b> relative memory savings on the quantized components, providing a practical pathway toward scalable low-bit embodied intelligence under strict compute, memory, and power constraints."
          data-zh="è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹å°†æ„ŸçŸ¥ã€è¯­è¨€å’Œæ§åˆ¶ç»Ÿä¸€ç”¨äºå…·èº«æ™ºèƒ½ä½“ï¼Œä½†ç”±äºè®¡ç®—å’Œå†…å­˜éœ€æ±‚çš„å¿«é€Ÿå¢é•¿ï¼Œå°¤å…¶æ˜¯æ¨¡å‹æ‰©å±•åˆ°æ›´é•¿çš„æ—¶é—´è·¨åº¦å’Œæ›´å¤§çš„éª¨å¹²ç½‘ç»œæ—¶ï¼Œåœ¨å®é™…éƒ¨ç½²ä¸­é¢ä¸´å·¨å¤§æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›ç“¶é¢ˆï¼Œæˆ‘ä»¬æå‡ºäº† <b>QuantVLA</b>â€”â€”ä¸€ä¸ªæ— éœ€è®­ç»ƒçš„è®­ç»ƒåé‡åŒ–ï¼ˆPTQï¼‰æ¡†æ¶ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯é¦–ä¸ªé¢å‘ VLA ç³»ç»Ÿçš„ PTQ æ–¹æ³•ï¼Œä¹Ÿæ˜¯é¦–ä¸ªæˆåŠŸé‡åŒ–æ‰©æ•£ Transformerï¼ˆDiTï¼‰åŠ¨ä½œå¤´çš„æ–¹æ³•ã€‚<b>QuantVLA</b> åŒ…å«ä¸‰ä¸ªå°ºåº¦æ ¡å‡†ç»„ä»¶ï¼šï¼ˆ1ï¼‰é€‰æ‹©æ€§é‡åŒ–å¸ƒå±€ï¼Œå°†è¯­è¨€éª¨å¹²å’Œ DiT ä¸­çš„æ‰€æœ‰çº¿æ€§å±‚æ•´æ•°åŒ–ï¼ŒåŒæ—¶ä¿æŒæ³¨æ„åŠ›æŠ•å½±ä¸ºæµ®ç‚¹æ•°ä»¥ä¿ç•™åŸå§‹ç®—å­è°ƒåº¦ï¼›ï¼ˆ2ï¼‰æ³¨æ„åŠ›æ¸©åº¦åŒ¹é…ï¼Œä¸€ç§è½»é‡çº§çš„é€å¤´ç¼©æ”¾æœºåˆ¶ï¼Œç”¨äºç¨³å®šæ³¨æ„åŠ› logits å¹¶åœ¨æ¨ç†æ—¶æŠ˜å åˆ°åé‡åŒ–ç¼©æ”¾å› å­ä¸­ï¼›ï¼ˆ3ï¼‰è¾“å‡ºå¤´å¹³è¡¡ï¼Œä¸€ç§é€å±‚æ®‹å·®æ¥å£æ ¡å‡†ï¼Œç”¨äºç¼“è§£æŠ•å½±åçš„èƒ½é‡æ¼‚ç§»ã€‚è¯¥æ¡†æ¶æ— éœ€é¢å¤–è®­ç»ƒï¼Œä»…ä½¿ç”¨å°‘é‡æ— æ ‡æ³¨çš„æ ¡å‡†ç¼“å†²åŒºï¼Œæ”¯æŒä½æ¯”ç‰¹æƒé‡å’Œæ¿€æ´»çš„æ•´æ•°æ ¸ï¼ŒåŒæ—¶ä¿æŒæ¶æ„ä¸å˜ã€‚åœ¨ LIBERO åŸºå‡†ä¸Šçš„ä»£è¡¨æ€§ VLA æ¨¡å‹ä¸Šï¼Œ<b>QuantVLA</b> è¶…è¿‡äº†å…¨ç²¾åº¦åŸºçº¿çš„ä»»åŠ¡æˆåŠŸç‡ï¼Œåœ¨é‡åŒ–ç»„ä»¶ä¸Šå®ç°äº†çº¦ <b>70%</b> çš„ç›¸å¯¹å†…å­˜èŠ‚çœï¼Œä¸ºåœ¨ä¸¥æ ¼çš„è®¡ç®—ã€å†…å­˜å’ŒåŠŸè€—çº¦æŸä¸‹å®ç°å¯æ‰©å±•çš„ä½æ¯”ç‰¹å…·èº«æ™ºèƒ½æä¾›äº†ä¸€æ¡å¯è¡Œè·¯å¾„ã€‚">
          Vision-language-action (VLA) models unify perception, language, and control for embodied agents but face
          significant challenges in practical deployment due to rapidly increasing compute and memory demands,
          especially as models scale to longer horizons and larger backbones. To address these bottlenecks, we
          introduce <b>QuantVLA</b>, a training-free post-training quantization (PTQ) framework that, to our
          knowledge, is the first PTQ approach for VLA systems and the first to successfully quantize a diffusion
          transformer (DiT) action head. <b>QuantVLA</b> incorporates three scale-calibrated components: (1) a
          selective quantization layout that integerizes all linear layers in both the language backbone and the
          DiT while keeping attention projections in floating point to preserve the original operator schedule; (2)
          attention temperature matching, a lightweight per-head scaling mechanism that stabilizes attention logits
          and is folded into the dequantization scales at inference; and (3) output head balancing, a per-layer
          residual interface calibration that mitigates post-projection energy drift. The framework requires no
          additional training, uses only a small unlabeled calibration buffer, and supports integer kernels for
          low-bit weights and activations while leaving the architecture unchanged. Across representative VLA
          models on LIBERO, <b>QuantVLA</b> exceeds the task success rates of fully-precision baselines, achieves
          about <b>70%</b> relative memory savings on the quantized components, providing a practical pathway
          toward scalable low-bit embodied intelligence under strict compute, memory, and power constraints.
        </p>
      </div>
    </div>
  </section>

  <!-- ===== Method ===== -->
  <section class="content-section" id="method">
    <div class="section-inner wide">
      <h2 class="section-title" data-en="Method" data-zh="æ–¹æ³•">Method</h2>
      <p class="method-text"
        data-en="Qualitative and quantitative results demonstrating the efficiency and accuracy of QuantVLA will be presented here. The method relies on post-training quantization to reduce model size while maintaining VLA capabilities."
        data-zh="å®šæ€§å’Œå®šé‡ç»“æœå±•ç¤ºäº† QuantVLA çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚è¯¥æ–¹æ³•ä¾èµ–è®­ç»ƒåé‡åŒ–æ¥å‡å°‘æ¨¡å‹å¤§å°ï¼ŒåŒæ—¶ä¿æŒ VLA èƒ½åŠ›ã€‚">
        Qualitative and quantitative results demonstrating the efficiency and accuracy of QuantVLA will be
        presented here. The method relies on post-training quantization to reduce model size while maintaining
        VLA capabilities.
      </p>
      <img src="./image/pipeline1.svg" alt="Method Overview" class="method-img">
    </div>
  </section>

  <!-- ===== Memory Efficiency ===== -->
  <section class="content-section" id="memory">
    <div class="section-inner wide">
      <h2 class="section-title" data-en="Memory Efficiency" data-zh="å†…å­˜æ•ˆç‡">Memory Efficiency</h2>
      <img src="./image/memory_comparison.png" alt="Memory Comparison: Baseline vs QuantVLA" class="section-img">
    </div>
  </section>

  <!-- ===== Evaluation Leaderboard ===== -->
  <section class="content-section" id="leaderboard">
    <div class="section-inner wide">
      <h2 class="section-title" data-en="Leaderboard" data-zh="æ’è¡Œæ¦œ">Leaderboard</h2>
      <img src="./image/leaderboard.png" alt="Evaluation Leaderboard" class="section-img">
    </div>
  </section>

  <!-- ===== Comparison ===== -->
  <section class="content-section" id="comparison">
    <div class="section-inner wide">
      <h2 class="section-title" data-en="Comparison of VLA Efficiency Paradigms" data-zh="VLA æ•ˆç‡èŒƒå¼å¯¹æ¯”">Comparison of VLA
        Efficiency Paradigms</h2>
      <img src="./image/comparison.png" alt="Efficiency Comparison" class="section-img">
    </div>
  </section>


  <!-- ===== Conclusion ===== -->
  <section class="content-section" id="conclusion">
    <div class="section-inner">
      <h2 class="section-title" data-en="Conclusion" data-zh="ç»“è®º">Conclusion</h2>
      <div class="abstract-box">
        <p data-en="We present <b>QuantVLA</b>, the <b>first PTQ framework for VLA models</b> that surpasses full precision baselines without any additional training. Using a <b>selective layout</b>, it integerizes the language backbone and the feedforward blocks of the diffusion transformer while attention projections remain in floating point. Two lightweight calibration scalars align the <b>attention temperature</b> and restore the <b>output energy</b>, thereby stabilizing low-bit inference. As a result, QuantVLA <b>reduces memory usage</b> and <b>improves accuracy</b>. Overall, QuantVLA is <b>training-free</b>, preserves the original architecture, and is robust across modalities, offering a practical path to <b>low-bit deployment</b> and laying the groundwork for future advances, lower power budgets, and reliable long-horizon generation."
          data-zh="æˆ‘ä»¬æå‡ºäº† <b>QuantVLA</b>ï¼Œå®ƒæ˜¯<b>é¦–ä¸ªé¢å‘ VLA æ¨¡å‹çš„ PTQ æ¡†æ¶</b>ï¼Œåœ¨æ— éœ€é¢å¤–è®­ç»ƒçš„æƒ…å†µä¸‹è¶…è¶Šäº†å…¨ç²¾åº¦åŸºçº¿ã€‚é€šè¿‡<b>é€‰æ‹©æ€§å¸ƒå±€</b>ï¼Œå®ƒå°†è¯­è¨€éª¨å¹²å’Œæ‰©æ•£ Transformer çš„å‰é¦ˆå—æ•´æ•°åŒ–ï¼ŒåŒæ—¶ä¿æŒæ³¨æ„åŠ›æŠ•å½±ä¸ºæµ®ç‚¹æ•°ã€‚ä¸¤ä¸ªè½»é‡çº§æ ¡å‡†æ ‡é‡å¯¹é½äº†<b>æ³¨æ„åŠ›æ¸©åº¦</b>å¹¶æ¢å¤äº†<b>è¾“å‡ºèƒ½é‡</b>ï¼Œä»è€Œç¨³å®šäº†ä½æ¯”ç‰¹æ¨ç†ã€‚å› æ­¤ï¼ŒQuantVLA <b>å‡å°‘äº†å†…å­˜ä½¿ç”¨</b>å¹¶<b>æå‡äº†ç²¾åº¦</b>ã€‚æ€»çš„æ¥è¯´ï¼ŒQuantVLA æ˜¯<b>æ— éœ€è®­ç»ƒçš„</b>ï¼Œä¿ç•™äº†åŸå§‹æ¶æ„ï¼Œå¹¶ä¸”åœ¨å¤šæ¨¡æ€ä¸‹å…·æœ‰é²æ£’æ€§ï¼Œä¸º<b>ä½æ¯”ç‰¹éƒ¨ç½²</b>æä¾›äº†ä¸€æ¡å¯è¡Œè·¯å¾„ï¼Œå¹¶ä¸ºæœªæ¥çš„è¿›æ­¥ã€æ›´ä½çš„åŠŸè€—é¢„ç®—å’Œå¯é çš„é•¿æ—¶é—´ç”Ÿæˆå¥ å®šäº†åŸºç¡€ã€‚">
          We present <b>QuantVLA</b>, the <b>first PTQ framework for VLA models</b> that surpasses full precision
          baselines without any additional training. Using a <b>selective layout</b>, it integerizes the language
          backbone and the feedforward blocks of the diffusion transformer while attention projections remain in
          floating point. Two lightweight calibration scalars align the <b>attention temperature</b> and restore
          the <b>output energy</b>, thereby stabilizing low-bit inference. As a result, QuantVLA <b>reduces memory
            usage</b> and <b>improves accuracy</b>. Overall, QuantVLA is <b>training-free</b>, preserves the
          original architecture, and is robust across modalities, offering a practical path to <b>low-bit
            deployment</b> and laying the groundwork for future advances, lower power budgets, and reliable
          long-horizon generation.
        </p>
      </div>
    </div>
  </section>

  <!-- ===== BibTeX ===== -->
  <section class="bibtex-section" id="bibtex">
    <div class="bibtex-inner">
      <h2 class="section-title">BibTeX</h2>
      <div class="bibtex-box">
        <code>@article{quantvla2026,
  author    = {Anonymous},
  title     = {QuantVLA: Quantizing Vision-Language-Action Models},
  journal   = {CVPR Submission},
  year      = {2026},
}</code>
      </div>
    </div>
  </section>

  <!-- ===== Footer ===== -->
  <footer class="site-footer">
    <div class="footer-icons">
      <a href="#"><i class="fas fa-file-pdf"></i></a>
      <a href="https://github.com/AIoT-MLSys-Lab/QuantVLA"><i class="fab fa-github"></i></a>
    </div>
    <p>
      This website is licensed under a
      <a href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 License</a>.
      Template adapted from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>.
    </p>
  </footer>

  <!-- ===== Scripts ===== -->
  <script>
    // Navbar scroll effect
    window.addEventListener('scroll', function () {
      const nav = document.getElementById('siteNav');
      if (window.scrollY > 20) {
        nav.classList.add('scrolled');
      } else {
        nav.classList.remove('scrolled');
      }
    });

    // Language switcher
    (function () {
      const btn = document.getElementById('langToggle');
      let lang = 'en';

      btn.addEventListener('click', function () {
        lang = lang === 'en' ? 'zh' : 'en';
        btn.innerHTML = '<i class="fas fa-globe"></i> ' + (lang === 'en' ? 'EN' : 'ä¸­æ–‡');
        document.documentElement.lang = lang === 'en' ? 'en' : 'zh-CN';

        document.querySelectorAll('[data-en][data-zh]').forEach(function (el) {
          el.innerHTML = el.getAttribute('data-' + lang);
        });
      });
    })();
  </script>

</body>

</html>