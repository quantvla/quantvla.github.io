<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="description"
    content="QuantVLA: The first post-training quantization framework for Vision-Language-Action models.">
  <meta name="keywords" content="QuantVLA, Robotics, VLA, Quantization, PTQ, DiT">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>QuantVLA: Quantizing Vision-Language-Action Models</title>

  <!-- Fonts -->
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800;900&display=swap"
    rel="stylesheet">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.0/css/all.min.css">

  <!-- Styles -->
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./image/icon.png">
</head>

<body>

  <!-- ===== Sticky Navbar ===== -->
  <nav class="site-nav" id="siteNav">
    <div class="nav-inner">
      <a href="#" class="nav-brand">
        <img src="./image/icon.png" alt="QuantVLA">
        <span>QuantVLA</span>
      </a>
      <ul class="nav-links">
        <li><a href="#abstract">Abstract</a></li>
        <li><a href="#method">Method</a></li>
        <li><a href="#leaderboard">Leaderboard</a></li>
        <li><a href="#comparison">Comparison</a></li>
        <li><a href="#bibtex">BibTeX</a></li>
      </ul>
    </div>
  </nav>

  <!-- ===== Hero Section ===== -->
  <section class="hero-main">
    <div class="hero-content">

      <!-- "First" Badge -->
      <div class="first-badge">üèÜ First Post-Training Quantization for VLA Systems</div>

      <!-- Large Icon + QuantVLA Title (like MMDR reference) -->
      <div class="hero-logo-block">
        <img src="./image/icon.png" alt="QuantVLA Icon">
        <span class="hero-logo-text">QuantVLA</span>
      </div>

      <p class="hero-subtitle">
        Scale-Calibrated Post-Training Quantization for<br>
        Vision-Language-Action Models
      </p>

      <!-- Authors -->
      <div class="hero-authors">
        <a href="#">Jingxuan Zhang</a><sup>2‚Ä†</sup>,
        <a href="#">Yunta Hsieh</a><sup>3‚Ä†</sup>,
        <a href="#">Zhongwei Wan</a><sup>1</sup>,
        <a href="#">Haokun Lin</a><sup>4</sup>,
        <a href="#">Xin Wang</a><sup>1</sup>,
        <a href="#">Ziqi Wang</a><sup>1</sup>,
        <a href="#">Yingtie Lei</a><sup>1</sup>,
        <a href="#">Mi Zhang</a><sup>1</sup>*
      </div>
      <div class="hero-affiliations">
        <sup>1</sup>The Ohio State University,
        <sup>2</sup>Indiana University,
        <sup>3</sup>University of Michigan,
        <sup>4</sup>City University of Hong Kong
      </div>
      <div class="hero-note">
        <sup>‚Ä†</sup>Equal Contribution
      </div>
      <div style="color: #ffffff; font-size: 1rem; margin-bottom: 1.5rem;">
        * Corresponding Author: mizhang.1@osu.edu
      </div>

      <!-- Action Buttons -->
      <div class="hero-buttons">
        <a href="https://arxiv.org/pdf/2011.12948" class="hero-btn hero-btn-primary">
          <i class="fas fa-file-pdf"></i> Paper
        </a>
        <a href="https://arxiv.org/abs/2011.12948" class="hero-btn hero-btn-outline">
          <i class="ai ai-arxiv"></i> arXiv
        </a>
        <a href="https://github.com/AIoT-MLSys-Lab/QuantVLA" class="hero-btn hero-btn-outline">
          <i class="fab fa-github"></i> Code
        </a>
      </div>
    </div>
  </section>

  <!-- ===== Highlight Cards ===== -->
  <section class="highlights-section">
    <div class="highlights-grid">
      <div class="highlight-card">
        <div class="highlight-icon">üèÜ</div>
        <div class="highlight-title">First PTQ for VLA</div>
        <div class="highlight-desc">First post-training quantization framework for VLA systems and first to quantize a
          DiT action head</div>
      </div>
      <div class="highlight-card">
        <div class="highlight-icon">üíæ</div>
        <div class="highlight-title">~70% Memory Savings</div>
        <div class="highlight-desc">Significant memory reduction on quantized components with no architecture changes
          required</div>
      </div>
      <div class="highlight-card">
        <div class="highlight-icon">‚ö°</div>
        <div class="highlight-title">Training-Free</div>
        <div class="highlight-desc">Uses only a small unlabeled calibration buffer ‚Äî no additional training or
          fine-tuning needed</div>
      </div>
    </div>
  </section>

  <!-- ===== Video Demos ===== -->
  <section class="demo-section">
    <div class="demo-inner">
      <div class="demo-videos">
        <video autoplay muted loop playsinline>
          <source src="./video/simulation1.mp4" type="video/mp4">
        </video>
        <video autoplay muted loop playsinline>
          <source src="./video/simluation2.mp4" type="video/mp4">
        </video>
        <video autoplay muted loop playsinline>
          <source src="./video/simulation3.mp4" type="video/mp4">
        </video>
        <video autoplay muted loop playsinline>
          <source src="./video/simulation4.mp4" type="video/mp4">
        </video>
      </div>
      <p class="demo-caption">
        <span class="dnerf">QuantVLA</span> enables efficient robot learning through quantization.
      </p>
    </div>
  </section>

  <!-- ===== Abstract ===== -->
  <section class="content-section" id="abstract">
    <div class="section-inner">
      <h2 class="section-title">Abstract</h2>
      <div class="abstract-box">
        <p>
          Vision-language-action (VLA) models unify perception, language, and control for embodied agents but face
          significant challenges in practical deployment due to rapidly increasing compute and memory demands,
          especially as models scale to longer horizons and larger backbones. To address these bottlenecks, we
          introduce <b>QuantVLA</b>, a training-free post-training quantization (PTQ) framework that, to our
          knowledge, is the first PTQ approach for VLA systems and the first to successfully quantize a diffusion
          transformer (DiT) action head. <b>QuantVLA</b> incorporates three scale-calibrated components: (1) a
          selective quantization layout that integerizes all linear layers in both the language backbone and the
          DiT while keeping attention projections in floating point to preserve the original operator schedule; (2)
          attention temperature matching, a lightweight per-head scaling mechanism that stabilizes attention logits
          and is folded into the dequantization scales at inference; and (3) output head balancing, a per-layer
          residual interface calibration that mitigates post-projection energy drift. The framework requires no
          additional training, uses only a small unlabeled calibration buffer, and supports integer kernels for
          low-bit weights and activations while leaving the architecture unchanged. Across representative VLA
          models on LIBERO, <b>QuantVLA</b> exceeds the task success rates of fully-precision baselines, achieves
          about <b>70%</b> relative memory savings on the quantized components, providing a practical pathway
          toward scalable low-bit embodied intelligence under strict compute, memory, and power constraints.
        </p>
      </div>
    </div>
  </section>

  <!-- ===== Method ===== -->
  <section class="content-section" id="method">
    <div class="section-inner wide">
      <h2 class="section-title">Method</h2>
      <p class="method-text">
        Qualitative and quantitative results demonstrating the efficiency and accuracy of QuantVLA will be
        presented here. The method relies on post-training quantization to reduce model size while maintaining
        VLA capabilities.
      </p>
      <img src="./image/pipeline1.svg" alt="Method Overview" class="method-img">
    </div>
  </section>

  <!-- ===== Evaluation Leaderboard ===== -->
  <section class="content-section" id="leaderboard">
    <div class="section-inner wide">
      <h2 class="section-title">Leaderboard</h2>
      <img src="./image/leaderboard.png" alt="Evaluation Leaderboard" class="section-img">
    </div>
  </section>

  <!-- ===== Comparison ===== -->
  <section class="content-section" id="comparison">
    <div class="section-inner wide">
      <h2 class="section-title">Comparison of VLA Efficiency Paradigms</h2>
      <img src="./image/comparison.png" alt="Efficiency Comparison" class="section-img">
    </div>
  </section>

  <!-- ===== Related Links ===== -->
  <section class="content-section" id="related">
    <div class="section-inner">
      <h2 class="section-title">Related Links</h2>
      <p class="related-text">
        We build upon recent advances in Vision-Language Models (VLMs) and Model Quantization.
        Relevant works include OpenVLA, RT-2, and various post-training quantization methods.
      </p>
    </div>
  </section>

  <!-- ===== BibTeX ===== -->
  <section class="bibtex-section" id="bibtex">
    <div class="bibtex-inner">
      <h2 class="section-title">BibTeX</h2>
      <div class="bibtex-box">
        <code>@article{quantvla2026,
  author    = {Anonymous},
  title     = {QuantVLA: Quantizing Vision-Language-Action Models},
  journal   = {CVPR Submission},
  year      = {2026},
}</code>
      </div>
    </div>
  </section>

  <!-- ===== Footer ===== -->
  <footer class="site-footer">
    <div class="footer-icons">
      <a href="#"><i class="fas fa-file-pdf"></i></a>
      <a href="https://github.com/AIoT-MLSys-Lab/QuantVLA"><i class="fab fa-github"></i></a>
    </div>
    <p>
      This website is licensed under a
      <a href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 License</a>.
      Template adapted from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>.
    </p>
  </footer>

  <!-- ===== Navbar scroll effect ===== -->
  <script>
    window.addEventListener('scroll', function () {
      const nav = document.getElementById('siteNav');
      if (window.scrollY > 20) {
        nav.classList.add('scrolled');
      } else {
        nav.classList.remove('scrolled');
      }
    });
  </script>

</body>

</html>