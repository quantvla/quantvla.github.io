<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="description"
    content="QuantVLA: The first post-training quantization framework for Vision-Language-Action models.">
  <meta name="keywords" content="QuantVLA, Robotics, VLA, Quantization, PTQ, DiT">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>QuantVLA: Quantizing Vision-Language-Action Models</title>

  <!-- Fonts -->
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800;900&display=swap"
    rel="stylesheet">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.0/css/all.min.css">

  <!-- Styles -->
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./image/icon.png">
</head>

<body>

  <!-- ===== Sticky Navbar ===== -->
  <nav class="site-nav" id="siteNav">
    <div class="nav-inner">
      <a href="#" class="nav-brand">
        <img src="./image/icon.png" alt="QuantVLA">
        <span>QuantVLA</span>
      </a>
      <ul class="nav-links">
        <li><a href="#abstract" data-en="Abstract" data-zh="摘要">Abstract</a></li>
        <li><a href="#leaderboard" data-en="Leaderboard" data-zh="排行榜">Leaderboard</a></li>
        <li><a href="#method" data-en="Method" data-zh="方法">Method</a></li>
        <li><a href="#memory" data-en="Memory" data-zh="内存">Memory</a></li>
        <li><a href="#comparison" data-en="Comparison" data-zh="对比">Comparison</a></li>
        <li><a href="#bibtex">BibTeX</a></li>
        <li>
          <button id="langToggle" class="lang-btn" title="Switch Language">
            <i class="fas fa-globe"></i> EN
          </button>
        </li>
      </ul>
    </div>
  </nav>

  <!-- ===== Hero Section ===== -->
  <section class="hero-main">
    <div class="hero-content">

      <!-- "First" Badge -->
      <div class="first-badge">🏆 First Post-Training Quantization for VLA Systems</div>

      <!-- Large Icon + QuantVLA Title -->
      <div class="hero-logo-block">
        <img src="./image/icon.png" alt="QuantVLA Icon">
        <span class="hero-logo-text">QuantVLA</span>
      </div>

      <p class="hero-subtitle">
        Scale-Calibrated Post-Training Quantization for<br>
        Vision-Language-Action Models
      </p>

      <!-- Authors -->
      <div class="hero-authors">
        <a href="#">Jingxuan Zhang</a><sup>2†</sup>,
        <a href="#">Yunta Hsieh</a><sup>3†</sup>,
        <a href="#">Zhongwei Wan</a><sup>1</sup>,
        <a href="#">Haokun Lin</a><sup>4</sup>,
        <a href="#">Xin Wang</a><sup>1</sup>,
        <a href="#">Ziqi Wang</a><sup>1</sup>,
        <a href="#">Yingtie Lei</a><sup>1</sup>,
        <a href="#">Mi Zhang</a><sup>1</sup>*
      </div>
      <div class="hero-affiliations">
        <sup>1</sup>The Ohio State University,
        <sup>2</sup>Indiana University,
        <sup>3</sup>University of Michigan,
        <sup>4</sup>City University of Hong Kong
      </div>
      <div class="hero-note">
        <sup>†</sup>Equal Contribution
      </div>
      <div style="color: #ffffff; font-size: 1rem; margin-bottom: 1.5rem;">
        * Corresponding Author: mizhang.1@osu.edu
      </div>

      <!-- Action Buttons -->
      <div class="hero-buttons">
        <a href="https://arxiv.org/pdf/2011.12948" class="hero-btn hero-btn-primary" target="_blank">
          <i class="fas fa-file-pdf"></i> Paper
        </a>
        <a href="https://arxiv.org/abs/2011.12948" class="hero-btn hero-btn-outline" target="_blank">
          <i class="ai ai-arxiv"></i> arXiv
        </a>
        <a href="https://github.com/AIoT-MLSys-Lab/QuantVLA" class="hero-btn hero-btn-outline" target="_blank">
          <i class="fab fa-github"></i> Code
        </a>
      </div>
    </div>
  </section>

  <!-- ===== Highlight Cards ===== -->
  <section class="highlights-section">
    <div class="highlights-grid">
      <div class="highlight-card">
        <div class="highlight-icon">🏆</div>
        <div class="highlight-title" data-en="First PTQ for VLA" data-zh="首个 VLA 量化方案">First PTQ for VLA</div>
        <div class="highlight-desc"
          data-en="First post-training quantization framework for VLA systems and first to quantize a DiT action head"
          data-zh="首个面向 VLA 系统的训练后量化框架，也是首个成功量化 DiT 动作头的方法">First post-training quantization framework for VLA systems
          and first to quantize a
          DiT action head</div>
      </div>
      <div class="highlight-card">
        <div class="highlight-icon">💾</div>
        <div class="highlight-title" data-en="~70% Memory Savings" data-zh="~70% 内存节省">~70% Memory Savings</div>
        <div class="highlight-desc"
          data-en="Significant memory reduction on quantized components with no architecture changes required"
          data-zh="量化组件显著减少内存占用，无需任何架构修改">Significant memory reduction on quantized components with no architecture
          changes
          required</div>
      </div>
      <div class="highlight-card">
        <div class="highlight-icon">⚡</div>
        <div class="highlight-title" data-en="Training-Free" data-zh="无需训练">Training-Free</div>
        <div class="highlight-desc"
          data-en="Uses only a small unlabeled calibration buffer — no additional training or fine-tuning needed"
          data-zh="仅需少量无标注校准数据，无需额外训练或微调">Uses only a small unlabeled calibration buffer — no additional training or
          fine-tuning needed</div>
      </div>
    </div>
  </section>

  <!-- ===== Video Demos ===== -->
  <section class="demo-section">
    <div class="demo-inner">
      <div class="demo-videos">
        <video autoplay muted loop playsinline>
          <source src="./video/simulation1.mp4" type="video/mp4">
        </video>
        <video autoplay muted loop playsinline>
          <source src="./video/simluation2.mp4" type="video/mp4">
        </video>
        <video autoplay muted loop playsinline>
          <source src="./video/simulation3.mp4" type="video/mp4">
        </video>
        <video autoplay muted loop playsinline>
          <source src="./video/simulation4.mp4" type="video/mp4">
        </video>
      </div>
      <p class="demo-caption"
        data-en="<span class='dnerf'>QuantVLA</span> enables efficient robot learning through quantization."
        data-zh="<span class='dnerf'>QuantVLA</span> 通过量化技术实现高效的机器人学习。">
        <span class="dnerf">QuantVLA</span> enables efficient robot learning through quantization.
      </p>
    </div>
  </section>

  <!-- ===== Abstract ===== -->
  <section class="content-section" id="abstract">
    <div class="section-inner">
      <h2 class="section-title" data-en="Abstract" data-zh="摘要">Abstract</h2>
      <div class="abstract-box">
        <p data-en="Vision-language-action (VLA) models unify perception, language, and control for embodied agents but face significant challenges in practical deployment due to rapidly increasing compute and memory demands, especially as models scale to longer horizons and larger backbones. To address these bottlenecks, we introduce <b>QuantVLA</b>, a training-free post-training quantization (PTQ) framework that, to our knowledge, is the first PTQ approach for VLA systems and the first to successfully quantize a diffusion transformer (DiT) action head. <b>QuantVLA</b> incorporates three scale-calibrated components: (1) a selective quantization layout that integerizes all linear layers in both the language backbone and the DiT while keeping attention projections in floating point to preserve the original operator schedule; (2) attention temperature matching, a lightweight per-head scaling mechanism that stabilizes attention logits and is folded into the dequantization scales at inference; and (3) output head balancing, a per-layer residual interface calibration that mitigates post-projection energy drift. The framework requires no additional training, uses only a small unlabeled calibration buffer, and supports integer kernels for low-bit weights and activations while leaving the architecture unchanged. Across representative VLA models on LIBERO, <b>QuantVLA</b> exceeds the task success rates of fully-precision baselines, achieves about <b>70%</b> relative memory savings on the quantized components, providing a practical pathway toward scalable low-bit embodied intelligence under strict compute, memory, and power constraints."
          data-zh="视觉-语言-动作（VLA）模型将感知、语言和控制统一用于具身智能体，但由于计算和内存需求的快速增长，尤其是模型扩展到更长的时间跨度和更大的骨干网络时，在实际部署中面临巨大挑战。为了解决这些瓶颈，我们提出了 <b>QuantVLA</b>——一个无需训练的训练后量化（PTQ）框架。据我们所知，这是首个面向 VLA 系统的 PTQ 方法，也是首个成功量化扩散 Transformer（DiT）动作头的方法。<b>QuantVLA</b> 包含三个尺度校准组件：（1）选择性量化布局，将语言骨干和 DiT 中的所有线性层整数化，同时保持注意力投影为浮点数以保留原始算子调度；（2）注意力温度匹配，一种轻量级的逐头缩放机制，用于稳定注意力 logits 并在推理时折叠到反量化缩放因子中；（3）输出头平衡，一种逐层残差接口校准，用于缓解投影后的能量漂移。该框架无需额外训练，仅使用少量无标注的校准缓冲区，支持低比特权重和激活的整数核，同时保持架构不变。在 LIBERO 基准上的代表性 VLA 模型上，<b>QuantVLA</b> 超过了全精度基线的任务成功率，在量化组件上实现了约 <b>70%</b> 的相对内存节省，为在严格的计算、内存和功耗约束下实现可扩展的低比特具身智能提供了一条可行路径。">
          Vision-language-action (VLA) models unify perception, language, and control for embodied agents but face
          significant challenges in practical deployment due to rapidly increasing compute and memory demands,
          especially as models scale to longer horizons and larger backbones. To address these bottlenecks, we
          introduce <b>QuantVLA</b>, a training-free post-training quantization (PTQ) framework that, to our
          knowledge, is the first PTQ approach for VLA systems and the first to successfully quantize a diffusion
          transformer (DiT) action head. <b>QuantVLA</b> incorporates three scale-calibrated components: (1) a
          selective quantization layout that integerizes all linear layers in both the language backbone and the
          DiT while keeping attention projections in floating point to preserve the original operator schedule; (2)
          attention temperature matching, a lightweight per-head scaling mechanism that stabilizes attention logits
          and is folded into the dequantization scales at inference; and (3) output head balancing, a per-layer
          residual interface calibration that mitigates post-projection energy drift. The framework requires no
          additional training, uses only a small unlabeled calibration buffer, and supports integer kernels for
          low-bit weights and activations while leaving the architecture unchanged. Across representative VLA
          models on LIBERO, <b>QuantVLA</b> exceeds the task success rates of fully-precision baselines, achieves
          about <b>70%</b> relative memory savings on the quantized components, providing a practical pathway
          toward scalable low-bit embodied intelligence under strict compute, memory, and power constraints.
        </p>
      </div>
    </div>
  </section>


  <!-- ===== Evaluation Leaderboard ===== -->
  <section class="content-section" id="leaderboard">
    <div class="section-inner wide">
      <h2 class="section-title" data-en="Leaderboard" data-zh="排行榜">Leaderboard</h2>
      <p class="leaderboard-subtitle" data-en="LIBERO Benchmark — Success Rate (%)" data-zh="LIBERO 基准 — 成功率 (%)">LIBERO
        Benchmark — Success Rate (%)</p>

      <!-- Model Filter Dropdown -->
      <div class="lb-controls">
        <div class="lb-dropdown" id="lbDropdown">
          <button class="lb-dropdown-btn" id="lbDropdownBtn" data-en="All Models" data-zh="全部模型">
            All Models <span class="lb-arrow">▾</span>
          </button>
          <ul class="lb-dropdown-menu" id="lbDropdownMenu">
            <li class="lb-dropdown-item active" data-filter="all" data-en="All Models" data-zh="全部模型">All Models</li>
            <li class="lb-dropdown-item" data-filter="pi" data-en="π0.5" data-zh="π0.5">π0.5</li>
            <li class="lb-dropdown-item" data-filter="groot" data-en="GR00T N1.5" data-zh="GR00T N1.5">GR00T N1.5</li>
          </ul>
        </div>
        <div class="lb-methods">
          <label class="lb-check"><input type="checkbox" value="baseline" checked> <span data-en="Baseline (FP16)"
              data-zh="基线 (FP16)">Baseline (FP16)</span></label>
          <label class="lb-check"><input type="checkbox" value="smoothquant" checked> <span>SmoothQuant</span></label>
          <label class="lb-check"><input type="checkbox" value="duquant" checked> <span>DuQuant</span></label>
          <label class="lb-check"><input type="checkbox" value="quantvla" checked> <span>QuantVLA</span></label>
        </div>
      </div>

      <!-- Interactive Table -->
      <div class="lb-table-wrap">
        <table class="lb-table" id="lbTable">
          <thead>
            <tr>
              <th data-en="Base Model" data-zh="基础模型">Base Model</th>
              <th class="lb-sortable" data-col="1" data-en="Method" data-zh="方法">Method <span class="sort-icon">⇅</span>
              </th>
              <th data-en="Precision" data-zh="精度">Precision</th>
              <th data-en="Layer Selection" data-zh="层选择">Layer Selection</th>
              <th class="lb-sortable" data-col="4" data-en="Spatial" data-zh="空间">Spatial <span
                  class="sort-icon">⇅</span></th>
              <th class="lb-sortable" data-col="5" data-en="Object" data-zh="物体">Object <span class="sort-icon">⇅</span>
              </th>
              <th class="lb-sortable" data-col="6" data-en="Goal" data-zh="目标">Goal <span class="sort-icon">⇅</span>
              </th>
              <th class="lb-sortable" data-col="7" data-en="Long" data-zh="长序列">Long <span class="sort-icon">⇅</span>
              </th>
              <th class="lb-sortable" data-col="8" data-en="Avg" data-zh="平均">Avg <span class="sort-icon">⇅</span></th>
            </tr>
          </thead>
          <tbody>
            <!-- π0.5 rows -->
            <tr data-model="pi" data-method="baseline">
              <td><span class="lb-badge lb-badge-pi">π0.5</span></td>
              <td>Baseline</td>
              <td>FP16</td>
              <td>–</td>
              <td>98.50</td>
              <td>99.00</td>
              <td>97.50</td>
              <td>93.50</td>
              <td>97.10</td>
            </tr>
            <tr data-model="pi" data-method="smoothquant">
              <td><span class="lb-badge lb-badge-pi">π0.5</span></td>
              <td>+SmoothQuant</td>
              <td>W8A8</td>
              <td>LLM</td>
              <td>97.50</td>
              <td>98.50</td>
              <td>98.00</td>
              <td>92.50</td>
              <td>96.60</td>
            </tr>
            <tr data-model="pi" data-method="smoothquant">
              <td><span class="lb-badge lb-badge-pi">π0.5</span></td>
              <td>+SmoothQuant</td>
              <td>W8A8</td>
              <td>LLM + DiT(MLP)</td>
              <td>98.00</td>
              <td>99.00</td>
              <td>99.00</td>
              <td>92.00</td>
              <td>97.00</td>
            </tr>
            <tr data-model="pi" data-method="duquant">
              <td><span class="lb-badge lb-badge-pi">π0.5</span></td>
              <td>+DuQuant</td>
              <td>W4A8</td>
              <td>LLM</td>
              <td>98.00</td>
              <td>98.50</td>
              <td>97.50</td>
              <td>92.00</td>
              <td>96.50</td>
            </tr>
            <tr data-model="pi" data-method="duquant">
              <td><span class="lb-badge lb-badge-pi">π0.5</span></td>
              <td>+DuQuant</td>
              <td>W4A8</td>
              <td>LLM + DiT(MLP)</td>
              <td>98.00</td>
              <td>97.00</td>
              <td>94.50</td>
              <td>92.00</td>
              <td>95.40</td>
            </tr>
            <tr data-model="pi" data-method="quantvla" class="lb-highlight">
              <td><span class="lb-badge lb-badge-pi">π0.5</span></td>
              <td><strong>+QuantVLA</strong></td>
              <td>W4A8</td>
              <td>LLM</td>
              <td>98.50</td>
              <td>99.00</td>
              <td>96.50</td>
              <td>96.50</td>
              <td><strong>97.60</strong></td>
            </tr>
            <tr data-model="pi" data-method="quantvla" class="lb-highlight">
              <td><span class="lb-badge lb-badge-pi">π0.5</span></td>
              <td><strong>+QuantVLA</strong></td>
              <td>W4A8</td>
              <td>LLM + DiT(MLP)</td>
              <td>98.50</td>
              <td>98.00</td>
              <td>98.00</td>
              <td>96.00</td>
              <td><strong>97.60</strong></td>
            </tr>
            <!-- GR00T N1.5 rows -->
            <tr data-model="groot" data-method="baseline">
              <td><span class="lb-badge lb-badge-groot">GR00T</span></td>
              <td>Baseline</td>
              <td>FP16</td>
              <td>–</td>
              <td>92.00</td>
              <td>92.00</td>
              <td>86.00</td>
              <td>76.00</td>
              <td>86.50</td>
            </tr>
            <tr data-model="groot" data-method="duquant">
              <td><span class="lb-badge lb-badge-groot">GR00T</span></td>
              <td>+DuQuant</td>
              <td>W4A8</td>
              <td>LLM</td>
              <td>86.00</td>
              <td>92.00</td>
              <td>80.00</td>
              <td>80.00</td>
              <td>84.50</td>
            </tr>
            <tr data-model="groot" data-method="duquant">
              <td><span class="lb-badge lb-badge-groot">GR00T</span></td>
              <td>+DuQuant</td>
              <td>W4A8</td>
              <td>LLM + DiT</td>
              <td>66.00</td>
              <td>70.00</td>
              <td>68.00</td>
              <td>76.00</td>
              <td>70.00</td>
            </tr>
            <tr data-model="groot" data-method="quantvla" class="lb-highlight">
              <td><span class="lb-badge lb-badge-groot">GR00T</span></td>
              <td><strong>+QuantVLA</strong></td>
              <td>W4A8</td>
              <td>LLM</td>
              <td>96.00</td>
              <td>94.00</td>
              <td>92.00</td>
              <td>66.00</td>
              <td>87.00</td>
            </tr>
            <tr data-model="groot" data-method="quantvla" class="lb-highlight">
              <td><span class="lb-badge lb-badge-groot">GR00T</span></td>
              <td><strong>+QuantVLA</strong></td>
              <td>W4A8</td>
              <td>LLM + DiT</td>
              <td>96.00</td>
              <td>92.00</td>
              <td>90.00</td>
              <td>74.00</td>
              <td><strong>88.00</strong></td>
            </tr>
          </tbody>
        </table>
      </div>
    </div>
  </section>

  <!-- ===== Method ===== -->
  <section class="content-section" id="method">
    <div class="section-inner wide">
      <h2 class="section-title" data-en="Method" data-zh="方法">Method</h2>
      <p class="method-text"
        data-en="Qualitative and quantitative results demonstrating the efficiency and accuracy of QuantVLA will be presented here. The method relies on post-training quantization to reduce model size while maintaining VLA capabilities."
        data-zh="定性和定量结果展示了 QuantVLA 的效率和准确性。该方法依赖训练后量化来减少模型大小，同时保持 VLA 能力。">
        Qualitative and quantitative results demonstrating the efficiency and accuracy of QuantVLA will be
        presented here. The method relies on post-training quantization to reduce model size while maintaining
        VLA capabilities.
      </p>
      <img src="./image/pipeline1.svg" alt="Method Overview" class="method-img">
    </div>
  </section>

  <!-- ===== Memory Efficiency ===== -->
  <section class="content-section" id="memory">
    <div class="section-inner wide">
      <h2 class="section-title" data-en="Memory Efficiency" data-zh="内存效率">Memory Efficiency</h2>
      <img src="./image/memory_comparison.png" alt="Memory Comparison: Baseline vs QuantVLA" class="section-img">
    </div>
  </section>

  <!-- ===== Comparison ===== -->
  <section class="content-section" id="comparison">
    <div class="section-inner wide">
      <h2 class="section-title" data-en="Comparison of VLA Efficiency Paradigms" data-zh="VLA 效率范式对比">Comparison of VLA
        Efficiency Paradigms</h2>
      <img src="./image/comparison.png" alt="Efficiency Comparison" class="section-img">
    </div>
  </section>


  <!-- ===== Conclusion ===== -->
  <section class="content-section" id="conclusion">
    <div class="section-inner">
      <h2 class="section-title" data-en="Conclusion" data-zh="结论">Conclusion</h2>
      <div class="abstract-box">
        <p data-en="We present <b>QuantVLA</b>, the <b>first PTQ framework for VLA models</b> that surpasses full precision baselines without any additional training. Using a <b>selective layout</b>, it integerizes the language backbone and the feedforward blocks of the diffusion transformer while attention projections remain in floating point. Two lightweight calibration scalars align the <b>attention temperature</b> and restore the <b>output energy</b>, thereby stabilizing low-bit inference. As a result, QuantVLA <b>reduces memory usage</b> and <b>improves accuracy</b>. Overall, QuantVLA is <b>training-free</b>, preserves the original architecture, and is robust across modalities, offering a practical path to <b>low-bit deployment</b> and laying the groundwork for future advances, lower power budgets, and reliable long-horizon generation."
          data-zh="我们提出了 <b>QuantVLA</b>，它是<b>首个面向 VLA 模型的 PTQ 框架</b>，在无需额外训练的情况下超越了全精度基线。通过<b>选择性布局</b>，它将语言骨干和扩散 Transformer 的前馈块整数化，同时保持注意力投影为浮点数。两个轻量级校准标量对齐了<b>注意力温度</b>并恢复了<b>输出能量</b>，从而稳定了低比特推理。因此，QuantVLA <b>减少了内存使用</b>并<b>提升了精度</b>。总的来说，QuantVLA 是<b>无需训练的</b>，保留了原始架构，并且在多模态下具有鲁棒性，为<b>低比特部署</b>提供了一条可行路径，并为未来的进步、更低的功耗预算和可靠的长时间生成奠定了基础。">
          We present <b>QuantVLA</b>, the <b>first PTQ framework for VLA models</b> that surpasses full precision
          baselines without any additional training. Using a <b>selective layout</b>, it integerizes the language
          backbone and the feedforward blocks of the diffusion transformer while attention projections remain in
          floating point. Two lightweight calibration scalars align the <b>attention temperature</b> and restore
          the <b>output energy</b>, thereby stabilizing low-bit inference. As a result, QuantVLA <b>reduces memory
            usage</b> and <b>improves accuracy</b>. Overall, QuantVLA is <b>training-free</b>, preserves the
          original architecture, and is robust across modalities, offering a practical path to <b>low-bit
            deployment</b> and laying the groundwork for future advances, lower power budgets, and reliable
          long-horizon generation.
        </p>
      </div>
    </div>
  </section>

  <!-- ===== BibTeX ===== -->
  <section class="bibtex-section" id="bibtex">
    <div class="bibtex-inner">
      <h2 class="section-title">BibTeX</h2>
      <div class="bibtex-box">
        <code>@article{quantvla2026,
  author    = {Anonymous},
  title     = {QuantVLA: Quantizing Vision-Language-Action Models},
  journal   = {CVPR Submission},
  year      = {2026},
}</code>
      </div>
    </div>
  </section>

  <!-- ===== Footer ===== -->
  <footer class="site-footer"
    style="display:flex; justify-content:space-between; align-items:center; padding:1rem 2rem; font-size:0.9rem;">
    <span>© 2026 QuantVLA</span>
    <a href="https://github.com/AIoT-MLSys-Lab/QuantVLA" style="color:inherit; text-decoration:none;"
      target="_blank">GitHub Pages</a>
  </footer>

  <!-- ===== Scripts ===== -->
  <script>
    // Navbar scroll effect
    window.addEventListener('scroll', function () {
      const nav = document.getElementById('siteNav');
      if (window.scrollY > 20) {
        nav.classList.add('scrolled');
      } else {
        nav.classList.remove('scrolled');
      }
    });

    // Language switcher
    (function () {
      const btn = document.getElementById('langToggle');
      let lang = 'en';

      btn.addEventListener('click', function () {
        lang = lang === 'en' ? 'zh' : 'en';
        btn.innerHTML = '<i class="fas fa-globe"></i> ' + (lang === 'en' ? 'EN' : '中文');
        document.documentElement.lang = lang === 'en' ? 'en' : 'zh-CN';

        document.querySelectorAll('[data-en][data-zh]').forEach(function (el) {
          el.innerHTML = el.getAttribute('data-' + lang);
        });
      });
    })();

    // Leaderboard interactivity
    (function () {
      const table = document.getElementById('lbTable');
      if (!table) return;
      const rows = Array.from(table.querySelectorAll('tbody tr'));
      const dropdown = document.getElementById('lbDropdown');
      const dropdownBtn = document.getElementById('lbDropdownBtn');
      const dropdownItems = document.querySelectorAll('.lb-dropdown-item');
      const checks = document.querySelectorAll('.lb-check input');
      let activeModel = 'all';

      function applyFilters() {
        const activeMethods = Array.from(checks).filter(c => c.checked).map(c => c.value);
        rows.forEach(function (row) {
          const model = row.getAttribute('data-model');
          const method = row.getAttribute('data-method');
          const modelMatch = activeModel === 'all' || model === activeModel;
          const methodMatch = activeMethods.includes(method);
          row.classList.toggle('lb-hidden', !(modelMatch && methodMatch));
        });
      }

      // Toggle dropdown
      dropdownBtn.addEventListener('click', function (e) {
        e.stopPropagation();
        dropdown.classList.toggle('open');
      });

      // Select item
      dropdownItems.forEach(function (item) {
        item.addEventListener('click', function () {
          dropdownItems.forEach(i => i.classList.remove('active'));
          item.classList.add('active');
          activeModel = item.getAttribute('data-filter');
          // Update button text
          const arrow = dropdownBtn.querySelector('.lb-arrow');
          dropdownBtn.textContent = item.textContent + ' ';
          dropdownBtn.appendChild(arrow);
          dropdown.classList.remove('open');
          applyFilters();
        });
      });

      // Close on outside click
      document.addEventListener('click', function () {
        dropdown.classList.remove('open');
      });

      checks.forEach(function (cb) {
        cb.addEventListener('change', applyFilters);
      });

      // Column sorting
      const methodOrder = { 'baseline': 0, '+smoothquant': 1, '+duquant': 2, '+quantvla': 3 };
      // Store original order for reset
      rows.forEach(function (r, i) { r.setAttribute('data-orig', i); });
      let sortCol = -1, sortAsc = true;
      table.querySelectorAll('th.lb-sortable').forEach(function (th) {
        th.addEventListener('click', function () {
          const col = parseInt(th.getAttribute('data-col'));
          const tbody = table.querySelector('tbody');

          // Clear previous highlight
          table.querySelectorAll('.lb-col-active').forEach(el => el.classList.remove('lb-col-active'));

          if (col === 1) {
            // Method column: reset to default order
            rows.sort(function (a, b) {
              return parseInt(a.getAttribute('data-orig')) - parseInt(b.getAttribute('data-orig'));
            });
            sortCol = -1;
            table.querySelectorAll('.sort-icon').forEach(s => s.textContent = '⇅');
          } else {
            if (sortCol === col) { sortAsc = !sortAsc; } else { sortCol = col; sortAsc = false; }
            rows.sort(function (a, b) {
              const na = parseFloat(a.children[col].textContent.trim());
              const nb = parseFloat(b.children[col].textContent.trim());
              return sortAsc ? na - nb : nb - na;
            });
            table.querySelectorAll('.sort-icon').forEach(s => s.textContent = '⇅');
            th.querySelector('.sort-icon').textContent = sortAsc ? '↑' : '↓';
            // Highlight active column
            th.classList.add('lb-col-active');
            rows.forEach(r => { if (r.children[col]) r.children[col].classList.add('lb-col-active'); });
          }
          rows.forEach(r => tbody.appendChild(r));
        });
      });
    })();
  </script>

</body>

</html>